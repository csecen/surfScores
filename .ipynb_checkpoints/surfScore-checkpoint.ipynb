{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSL Web Scraper\n",
    "\n",
    "#### This notebook is solely intended to scrape the WSL webpage to collect every wave scored from 2016-2018 and store this data in a separte file to be analyzed at a later time. The notebook relies on both BeautifulSoup and Selenium in order to scrape all the data necessary.\n",
    "\n",
    "#### The web scraping aspect of the notebook in multi-processed, allowing the scraping to be completed faster.\n",
    "<br>\n",
    "\n",
    "##### By: Connor Secen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports required\n",
    "import time\n",
    "import requests\n",
    "import multiprocessing\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# imports for webscraping\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "from selenium import webdriver\n",
    "import selenium.webdriver.support.expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraping itself takes up the most amount of time and the most amount of code for this project. There is one function called `scraper` that handles all of the web scraping. Each process calls this function with a different url, corresponding to a specific year. \n",
    "\n",
    "BeautifulSoup and Selenium are used in parallel in this project because both have there limitations. BeautifulSoup is very useful with parsing html and collecting all the information for the contents of the webpage, which is exactly what it is used for in this project. However, because the WSL webpage uses modals to store the information we are trying to collect, BeautifulSoup cannot access this information directly. This is where Selenium comes into play. Because Selenium has a webdriver that allows you to simulate a page, it can be used to navigate throughout the site in order to access the information that BeautifulSoup can then parse out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scraper(url, filename):\n",
    "    year = url[39:43]# get the year of the events\n",
    "    \n",
    "    roundNames = ['Round 1', 'Round 2', 'Round 3', 'Round 4', 'Round 5', 'Quarterfinals', 'Semifinals', 'Finals']\n",
    "    \n",
    "    r = requests.get(url)   # send request to the starting url\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    eventsRaw = soup.findAll('span', attrs={'class':'tour-event-name'})\n",
    "    events = [e.text for e in eventsRaw]   # collect all names of the events that happened in the corresponding year\n",
    "    \n",
    "    driver = webdriver.Chrome('/usr/bin/chromedriver')   # create an instance of a selenium web driver\n",
    "    driver.get(url)\n",
    "    \n",
    "    # loop through every event for the year and collect every wave \n",
    "    for event in events:\n",
    "        if event == 'Surf Ranch Pro':\n",
    "            continue\n",
    "            \n",
    "        #print('Starting new event')\n",
    "        # wait for the page to load, locate the next link to and move it into view\n",
    "        time.sleep(5)\n",
    "        element = driver.find_element_by_xpath(\"//a[@title=\\\"\" + event + \"\\\"]\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "        driver.execute_script(\"window.scrollBy(0, -100)\")\n",
    "        \n",
    "        # navigate to the results page for the current event\n",
    "        time.sleep(5)\n",
    "        element.click()\n",
    "        time.sleep(5)\n",
    "        driver.find_element_by_link_text('Results').click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        rounds = driver.find_elements_by_xpath(\"//a[@data-request-name='postEventWatch']\")\n",
    "        \n",
    "        if (year == '2018' and event == 'Billabong Pipe Masters'):\n",
    "            rounds = rounds[4:]\n",
    "            roundNames = ['Round 1', 'Round 2', 'Round 3', 'Round 4', 'Quarterfinals', 'Semifinals', 'Finals']\n",
    "        \n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", rounds[0])\n",
    "        driver.execute_script(\"window.scrollBy(0, -100)\")\n",
    "        time.sleep(5)\n",
    "        #driver.execute_script(\"window.scrollTo(0, 150)\")\n",
    "        for idx1, r in enumerate(rounds):\n",
    "            #print('Starting on the round')\n",
    "            rounds2 = driver.find_elements_by_xpath(\"//a[@data-request-name='postEventWatch']\")\n",
    "            \n",
    "            if (year == '2018' and event == 'Billabong Pipe Masters'):\n",
    "                rounds2 = rounds2[4:]\n",
    "            \n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", rounds2[idx1])\n",
    "            driver.execute_script(\"window.scrollBy(0, -100)\")\n",
    "            rounds2[idx1].click()\n",
    "            time.sleep(5)\n",
    "\n",
    "        \n",
    "            viewDetails = driver.find_elements_by_class_name('hot-heat__details-link')   # find all view details buttons\n",
    "            firstPassDone = False\n",
    "            # loop through all modals on the current pages and collect the all wave scores\n",
    "            for i, x in enumerate(viewDetails):\n",
    "                \n",
    "                #print('starting to collect heat scores')\n",
    "                x.click()   # open the scores for the heat\n",
    "                time.sleep(5)\n",
    "\n",
    "                source = driver.page_source # get page content\n",
    "                soup2 = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "                # collect raw scores, how many athletes are in the heat, and the shortened names of the athletes\n",
    "                rawScores = soup2.findAll(['span', 'div'], attrs={'class':['wave-score', 'wave--empty']})\n",
    "                rawCount = soup2.findAll('div', attrs={'class':'hot-heat__athletes'})\n",
    "                rawAthletes = soup2.findAll('div', attrs ={'class':'hot-heat-athlete__name--short'})\n",
    "\n",
    "                scores = [s.text for s in rawScores]   # clean the scores into just the text\n",
    "                # rawCount is a list of all instances of the number of surfers in each heat. The first subscript get the\n",
    "                # content. The '.attrs' gets the class names. The second subscript get the second class name which holds\n",
    "                # the number of surfer. The third subscript gets the last character which is the actual number. Then it\n",
    "                # is converted to an int\n",
    "                count = int(rawCount[0].attrs['class'][1][-1])\n",
    "                athletes = [a.text for a in rawAthletes]   # clean the athletes names into just text\n",
    "                \n",
    "                for x in range(0, count):\n",
    "                    \n",
    "                    score = scores[x::count]\n",
    "                    csvData = [year, athletes[x], event, roundNames[idx1], score]\n",
    "                    \n",
    "                    with open(filename, 'a') as csvFile:\n",
    "                        writer = csv.writer(csvFile)\n",
    "                        writer.writerow(csvData)\n",
    "\n",
    "                        csvFile.close()\n",
    "                    \n",
    "\n",
    "                time.sleep(5)\n",
    "                driver.find_element_by_xpath(\"//*[@class='close pass']\").click()   # close the current modal\n",
    "                time.sleep(5)\n",
    "                #print('finsihed collecting heat scores')\n",
    "                \n",
    "                index = i + 1\n",
    "                if index%4 == 0 and firstPassDone == True:\n",
    "                    driver.execute_script(\"window.scrollBy(0,500)\")\n",
    "                    #print(\"scrolling up\")\n",
    "                firstPassDone = True\n",
    "            #print('finishing the round')\n",
    "\n",
    "        # return to the list of events by going back twice\n",
    "        driver.back()\n",
    "        time.sleep(5)\n",
    "        driver.back()\n",
    "        time.sleep(2)\n",
    "        #print('Finishing event')\n",
    "    time.sleep(5)    \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `main` function of the program, the scraper function from above is called. Because there are 3 years of data that need to be collected, multiple processes are put to use to scrape each year seperately and make the scraping itself more efficient. There is one process per year of data being collected.\n",
    "\n",
    "Each process is writing to a separate csv file. Once each processes has completed, all 3 files will be concatinated into one single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    filenames = ['2016.csv', '2017.csv', '2018.csv']\n",
    "    structure = ['Year', 'Name', 'Event', 'Round', 'Scores']\n",
    "    \n",
    "    for file in filenames:\n",
    "        with open(file, 'w') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            writer.writerow(structure)\n",
    "\n",
    "            csvFile.close()\n",
    "    \n",
    "    # create the urls\n",
    "    url1 = 'https://www.worldsurfleague.com/events/2016/mct'\n",
    "    url2 = 'https://www.worldsurfleague.com/events/2017/mct'\n",
    "    url3 = 'https://www.worldsurfleague.com/events/2018/mct'\n",
    "    \n",
    "    # create the processes\n",
    "    p1 = multiprocessing.Process(target=scraper, args=(url1, filenames[0], )) \n",
    "    p2 = multiprocessing.Process(target=scraper, args=(url2, filenames[1], ))\n",
    "    p3 = multiprocessing.Process(target=scraper, args=(url3, filenames[2], ))\n",
    "    \n",
    "    # start the processes\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p3.start()\n",
    "    \n",
    "    \n",
    "    # wait for all processes to finish\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    p3.join()\n",
    "    \n",
    "    #combine all files in the list\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in filenames ])\n",
    "    #export to csv\n",
    "    combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
